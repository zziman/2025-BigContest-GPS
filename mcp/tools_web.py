# -*- coding: utf-8 -*-
# mcp/tools_web.py
import time, json, re
import urllib.parse as up
import requests
from typing import List, Dict, Any, Optional, Tuple
from .contracts import WebSearchInput, WebSearchOutput, WebDoc

# ğŸ”§ í”„ë¡œì íŠ¸ ì„¤ì •: secrets.toml > .env > default (app/config.pyì—ì„œ í†µì¼)
from my_agent.utils.config import (
    NAVER_CLIENT_ID, NAVER_CLIENT_SECRET,
    SURFER_API_KEY, TABILI_API_KEY,
    SEARCH_TIMEOUT, DEFAULT_TOPK, DEFAULT_RECENCY_DAYS
)

def _norm(text: Optional[str]) -> str:
    return (text or "").strip()

def _clip(s: str, n=300) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    return s[:n]

def _to_ts(published_at: str) -> float:
    """
    ë‹¤ì–‘í•œ í˜•ì‹(pubDate, ISO8601 ë“±)ì„ best-effortë¡œ epoch secondsë¡œ ë³€í™˜
    """
    s = _norm(published_at)
    if not s:
        return 0.0
    # 1) í”í•œ ISO8601: 2025-10-15T12:34:56
    for fmt in ("%Y-%m-%dT%H:%M:%S", "%Y-%m-%d %H:%M:%S"):
        try:
            return time.mktime(time.strptime(s[:19], fmt))
        except Exception:
            pass
    # 2) Naver pubDate: 'Wed, 16 Oct 2025 10:00:00 +0900'
    try:
        # %zëŠ” +0900 íŒŒì‹± ì§€ì› (Python 3.7+). time.mktimeì€ tz ë¯¸ì§€ì›ì´ë¼ ë³€í™˜ í›„ ë³´ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜
        # ì—¬ê¸°ì„  ì‹ ì„ ë„ í•„í„° ìš©ë„ë¼ ëŒ€ëµì  ë¹„êµë©´ ì¶©ë¶„.
        from email.utils import parsedate_to_datetime
        dt = parsedate_to_datetime(s)
        return dt.timestamp()
    except Exception:
        return 0.0

def _rank_blend(items: List[WebDoc]) -> List[WebDoc]:
    # ê°„ë‹¨ ë¸”ë Œë”©: score DESC â†’ published_at DESC â†’ title ASC
    return sorted(
        items,
        key=lambda x: (
            -float(x.get("score", 0) or 0),
            -_to_ts(x.get("published_at", "")),
            x.get("title", "") or ""
        )
    )

def _apply_recency_filter(items: List[WebDoc], recency_days: int) -> List[WebDoc]:
    if recency_days <= 0:
        return items
    cutoff = time.time() - recency_days * 86400
    out = []
    for d in items:
        ts = _to_ts(d.get("published_at", ""))
        if ts == 0.0 or ts >= cutoff:
            out.append(d)
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Providers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _naver_search(q: str, top_k: int) -> Tuple[str, List[WebDoc]]:
    if not (NAVER_CLIENT_ID and NAVER_CLIENT_SECRET):
        return "naver", []
    url = "https://openapi.naver.com/v1/search/news.json"
    # ë¸”ë¡œê·¸: https://openapi.naver.com/v1/search/blog.json
    params = {"query": q, "display": min(top_k, 10), "start": 1, "sort": "date"}
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    r = requests.get(url, params=params, headers=headers, timeout=SEARCH_TIMEOUT)
    r.raise_for_status()
    data = r.json().get("items", []) or []
    docs: List[WebDoc] = []
    for it in data:
        title = _norm(re.sub("<.*?>", "", it.get("title", "")))
        link = _norm(it.get("link"))
        snippet = _clip(re.sub("<.*?>", "", it.get("description", "")))
        origin = it.get("originallink") or link or ""
        source = up.urlparse(origin).netloc or up.urlparse(link or "").netloc
        pub = _norm(it.get("pubDate", ""))  # RFC822
        docs.append({
            "title": title,
            "url": link,
            "snippet": snippet,
            "source": source,
            "published_at": pub,
            "score": 0.7,  # ë² ì´ìŠ¤ ê°€ì¤‘ì¹˜
        })
    return "naver", docs

def _serper_search(q: str, top_k: int) -> Tuple[str, List[WebDoc]]:
    # Serper (ì„œí¼): https://google.serper.dev/search  (POST)
    if not SURFER_API_KEY:
        return "serper", []
    url = "https://google.serper.dev/search"
    headers = {"X-API-KEY": SURFER_API_KEY, "Content-Type": "application/json"}
    payload = {"q": q, "num": top_k}
    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=SEARCH_TIMEOUT)
    r.raise_for_status()
    j = r.json() or {}
    data = (j.get("news") or []) + (j.get("organic") or [])
    docs: List[WebDoc] = []
    for it in data[:top_k]:
        title = _norm(it.get("title"))
        link = _norm(it.get("link") or it.get("url"))
        snippet = _clip(it.get("snippet") or it.get("description") or "")
        source = up.urlparse(link or "").netloc
        pub = _norm(it.get("date") or it.get("publishedDate") or it.get("dateUtc") or "")
        docs.append({
            "title": title,
            "url": link,
            "snippet": snippet,
            "source": source,
            "published_at": pub,
            "score": 0.9,
        })
    return "serper", docs

def _tavily_search(q: str, top_k: int) -> Tuple[str, List[WebDoc]]:
    # Tavily (íƒ€ë¹Œë¦¬): https://api.tavily.com/search  (POST, Bearer)
    if not TABILI_API_KEY:
        return "tavily", []
    url = "https://api.tavily.com/search"
    headers = {"Authorization": f"Bearer {TABILI_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "query": q,
        "max_results": top_k,
        "search_depth": "advanced",   # í•„ìš” ì‹œ 'basic'
        "include_answer": False,
        "include_raw_content": False,
    }
    r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=SEARCH_TIMEOUT)
    r.raise_for_status()
    j = r.json() or {}
    data = j.get("results", []) or []
    docs: List[WebDoc] = []
    for it in data[:top_k]:
        link = _norm(it.get("url"))
        docs.append({
            "title": _norm(it.get("title")),
            "url": link,
            "snippet": _clip(it.get("content") or it.get("snippet") or ""),
            "source": up.urlparse(link or "").netloc,
            "published_at": _norm(it.get("published_time") or it.get("date") or ""),
            "score": 0.8,
        })
    return "tavily", docs

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Public MCP tool
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def web_search(query: str, provider: str = "auto",
               top_k: int = DEFAULT_TOPK,
               recency_days: int = DEFAULT_RECENCY_DAYS) -> WebSearchOutput:
    q = _norm(query)
    if not q:
        return {"success": False, "provider_used": provider, "count": 0, "docs": [], "error": "empty query"}

    providers = []
    if provider in ("auto", "naver"):  providers.append(_naver_search)
    if provider in ("auto", "serper", "surfer"): providers.append(_serper_search)  # 'surfer' ë³„ì¹­ í—ˆìš©
    if provider in ("auto", "tavily", "tabili"): providers.append(_tavily_search)  # 'tabili' ì˜¤íƒ€ ë³„ì¹­ í—ˆìš©

    merged: List[WebDoc] = []
    used: List[str] = []

    for fn in providers:
        try:
            name, docs = fn(q, top_k)
            if docs:
                used.append(name)
                merged.extend(docs)
        except Exception:
            # ê³µê¸‰ì í•˜ë‚˜ ì‹¤íŒ¨í•´ë„ ì „ì²´ ì‹¤íŒ¨ë¡œ ë§Œë“¤ì§€ ì•ŠìŒ
            continue

    if not merged:
        return {"success": False, "provider_used": ",".join(used) or provider, "count": 0, "docs": [], "error": "no results"}

    # ì‹ ì„ ë„ í•„í„° â†’ ë­í‚¹ ë¸”ë Œë”© â†’ ìƒìœ„ K
    merged = _apply_recency_filter(merged, recency_days)
    merged = _rank_blend(merged)[:top_k]

    return {"success": True, "provider_used": ",".join(used) or provider, "count": len(merged), "docs": merged, "error": None}
